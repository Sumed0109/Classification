{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Articles Identification\n",
    "\n",
    "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Given the abstract and title for a set of research articles, predict if the topic of article is related to computer science or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Details -\n",
    "\n",
    "ID - Unique ID for each article\n",
    "\n",
    "TITLE - Title of the research article\n",
    "\n",
    "ABSTRACT - Abstract of the research article\n",
    "\n",
    "Computer Science - Whether article belongs to topic computer science (1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import the necessary modelling algos.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#model selection\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "\n",
    "df = pd.read_csv(\"Articles.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \n",
       "0    Predictive models allow subject-specific inf...                 1  \n",
       "1    Rotation invariance and translation invarian...                 1  \n",
       "2    We introduce and develop the notion of spher...                 0  \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0  \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop - Ignore variable, if it is not significant.\n",
    "\n",
    "df = df.drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['title','abstract','ComputerScience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            20972 non-null  object\n",
      " 1   abstract         20972 non-null  object\n",
      " 2   ComputerScience  20972 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 491.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "abstract           0\n",
       "ComputerScience    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x15b0fec9700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.factorplot(data=df,kind='count',size=5,aspect=1,x='ComputerScience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ComputerScience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reconstructing subject-specific effect maps</td>\n",
       "      <td>predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rotation invariance neural network</td>\n",
       "      <td>rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spherical polyharmonics and poisson kernels fo...</td>\n",
       "      <td>we introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a finite element approximation for the stochas...</td>\n",
       "      <td>the stochastic landau--lifshitz--gilbert (ll...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comparative study of discrete wavelet transfor...</td>\n",
       "      <td>fourier-transform infra-red (ftir) spectra o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        reconstructing subject-specific effect maps   \n",
       "1                 rotation invariance neural network   \n",
       "2  spherical polyharmonics and poisson kernels fo...   \n",
       "3  a finite element approximation for the stochas...   \n",
       "4  comparative study of discrete wavelet transfor...   \n",
       "\n",
       "                                            abstract  ComputerScience  \n",
       "0    predictive models allow subject-specific inf...                1  \n",
       "1    rotation invariance and translation invarian...                1  \n",
       "2    we introduce and develop the notion of spher...                0  \n",
       "3    the stochastic landau--lifshitz--gilbert (ll...                0  \n",
       "4    fourier-transform infra-red (ftir) spectra o...                1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to lower case\n",
    "\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['abstract'] = df['abstract'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ComputerScience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reconstructing subject specific effect maps</td>\n",
       "      <td>predictive models allow subject specific inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rotation invariance neural network</td>\n",
       "      <td>rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spherical polyharmonics and poisson kernels fo...</td>\n",
       "      <td>we introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a finite element approximation for the stochas...</td>\n",
       "      <td>the stochastic landau  lifshitz  gilbert  ll...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comparative study of discrete wavelet transfor...</td>\n",
       "      <td>fourier transform infra red  ftir  spectra o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        reconstructing subject specific effect maps   \n",
       "1                 rotation invariance neural network   \n",
       "2  spherical polyharmonics and poisson kernels fo...   \n",
       "3  a finite element approximation for the stochas...   \n",
       "4  comparative study of discrete wavelet transfor...   \n",
       "\n",
       "                                            abstract  ComputerScience  \n",
       "0    predictive models allow subject specific inf...                1  \n",
       "1    rotation invariance and translation invarian...                1  \n",
       "2    we introduce and develop the notion of spher...                0  \n",
       "3    the stochastic landau  lifshitz  gilbert  ll...                0  \n",
       "4    fourier transform infra red  ftir  spectra o...                1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "\n",
    "df['title'] = df['title'].str.replace(\"[^a-zA-z]\", \" \") \n",
    "df['abstract'] = df['abstract'].str.replace(\"[^a-zA-z]\", \" \") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ComputerScience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reconstructing subject specific effect maps</td>\n",
       "      <td>predictive models allow subject specific infer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rotation invariance neural network</td>\n",
       "      <td>rotation invariance translation invariance hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spherical polyharmonics poisson kernels polyha...</td>\n",
       "      <td>introduce develop notion spherical polyharmoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finite element approximation stochastic maxwel...</td>\n",
       "      <td>stochastic landau lifshitz gilbert equation co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comparative study discrete wavelet transforms ...</td>\n",
       "      <td>fourier transform infra ftir spectra samples f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        reconstructing subject specific effect maps   \n",
       "1                 rotation invariance neural network   \n",
       "2  spherical polyharmonics poisson kernels polyha...   \n",
       "3  finite element approximation stochastic maxwel...   \n",
       "4  comparative study discrete wavelet transforms ...   \n",
       "\n",
       "                                            abstract  ComputerScience  \n",
       "0  predictive models allow subject specific infer...                1  \n",
       "1  rotation invariance translation invariance hav...                1  \n",
       "2  introduce develop notion spherical polyharmoni...                0  \n",
       "3  stochastic landau lifshitz gilbert equation co...                0  \n",
       "4  fourier transform infra ftir spectra samples f...                1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Short Words\n",
    "\n",
    "df['title'] = df['title'].apply(lambda old_string: ' '.join([w for w in old_string.split() if len(w)>3]))\n",
    "df['abstract'] = df['abstract'].apply(lambda old_string: ' '.join([w for w in old_string.split() if len(w)>3]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ComputerScience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reconstructing subject specific effect map</td>\n",
       "      <td>predictive model allow subject specific infere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rotation invariance neural network</td>\n",
       "      <td>rotation invariance translation invariance hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spherical polyharmonics poisson kernel polyhar...</td>\n",
       "      <td>introduce develop notion spherical polyharmoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finite element approximation stochastic maxwel...</td>\n",
       "      <td>stochastic landau lifshitz gilbert equation co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comparative study discrete wavelet transforms ...</td>\n",
       "      <td>fourier transform infra ftir spectrum sample f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         reconstructing subject specific effect map   \n",
       "1                 rotation invariance neural network   \n",
       "2  spherical polyharmonics poisson kernel polyhar...   \n",
       "3  finite element approximation stochastic maxwel...   \n",
       "4  comparative study discrete wavelet transforms ...   \n",
       "\n",
       "                                            abstract  ComputerScience  \n",
       "0  predictive model allow subject specific infere...                1  \n",
       "1  rotation invariance translation invariance hav...                1  \n",
       "2  introduce develop notion spherical polyharmoni...                0  \n",
       "3  stochastic landau lifshitz gilbert equation co...                0  \n",
       "4  fourier transform infra ftir spectrum sample f...                1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization words using NLTK\n",
    "\n",
    "wnlemma = WordNetLemmatizer()\n",
    "\n",
    "df['title'] = df['title'].apply(lambda text: ' '.join([wnlemma.lemmatize(word) for word in text.split()]))\n",
    "df['abstract'] = df['abstract'].apply(lambda text: ' '.join([wnlemma.lemmatize(word) for word in text.split()]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20972, 2)\n",
      "(20972,)\n"
     ]
    }
   ],
   "source": [
    "df_Y = df['ComputerScience'].values\n",
    "df_X = df.drop(['ComputerScience'], axis=1)\n",
    "\n",
    "print(df_X.shape)\n",
    "print(df_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---1.2 Splitting data into Train and cross validation(or test): Stratified Sampling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_X, df_Y, test_size=0.2, stratify=df_Y,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET 1 - Encoding text features using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(16777, 2000) (16777,)\n",
      "(4195, 2000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=10, max_features=2000)\n",
    "cv.fit(X_train['title'].values)\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_title_bow = cv.transform(X_train['title'].values)\n",
    "X_test_title_bow = cv.transform(X_test['title'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_title_bow.shape, Y_train.shape)\n",
    "print(X_test_title_bow.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(16777, 7000) (16777,)\n",
      "(4195, 7000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=10, max_features=7000,stop_words = 'english')\n",
    "cv.fit(X_train['abstract'].values)\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_abstract_bow = cv.transform(X_train['abstract'].values)\n",
    "X_test_abstract_bow = cv.transform(X_test['abstract'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_abstract_bow.shape, Y_train.shape)\n",
    "print(X_test_abstract_bow.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix 1\n",
      "(16777, 9000) (16777,)\n",
      "(4195, 9000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "# --- Concatinating all the features\n",
    "\n",
    "X_tr1 = hstack((X_train_title_bow, X_train_abstract_bow)).tocsr()\n",
    "X_te1 = hstack((X_test_title_bow, X_test_abstract_bow)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix 1\")\n",
    "print(X_tr1.shape, Y_train.shape)\n",
    "print(X_te1.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling BOW Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'weights': 'distance', 'n_neighbors': 6, 'n_jobs': -1, 'leaf_size': 3, 'algorithm': 'ball_tree'}\n"
     ]
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[5,6,7,8,9,10],\n",
    "          'leaf_size':[1,2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute']}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model11 = RandomizedSearchCV(knn1, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "model11.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model11.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6798569725864124\n"
     ]
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(weights = 'distance', n_neighbors =6, n_jobs= -1, leaf_size= 3, algorithm= 'ball_tree')\n",
    "\n",
    "knn1.fit(X_tr1,Y_train)\n",
    "\n",
    "knn1_pred = knn1.predict(X_te1)\n",
    "\n",
    "print(accuracy_score(Y_test,knn1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'penalty': 'l2', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "          'C': [0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model12 = RandomizedSearchCV(lr1, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "model12.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model12.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8481525625744935\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression(C = 0.1, penalty = 'l2')\n",
    "\n",
    "lr1.fit(X_tr1, Y_train)\n",
    "\n",
    "lr1_pred = lr1.predict(X_te1)\n",
    "\n",
    "print(accuracy_score(Y_test,lr1_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'kernel': 'rbf', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "svc1 = SVC()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'kernel': ['linear','rbf'], \n",
    "          'C' : [1,3]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model13 = RandomizedSearchCV(svc1, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "#Learning\n",
    "model13.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model13.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8638855780691299\n"
     ]
    }
   ],
   "source": [
    "svc1 = SVC(kernel = 'rbf', C = 1)\n",
    "\n",
    "#Learning\n",
    "svc1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "svc1_pred = svc1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,svc1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "bnb1 = BernoulliNB()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'alpha': [0.1, 0.25, 0.5,0.75, 1.0, 1.25 ]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model14 = RandomizedSearchCV(bnb1, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "#Learning\n",
    "model14.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model14.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8448152562574494\n"
     ]
    }
   ],
   "source": [
    "bnb1 = BernoulliNB(alpha = 0.1)\n",
    "\n",
    "#Learning\n",
    "bnb1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "bnb1_pred = bnb1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,bnb1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "dt1 = DecisionTreeClassifier()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model15 = RandomizedSearchCV(dt1, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model15.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model15.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7389749702026222\n"
     ]
    }
   ],
   "source": [
    "dt1 = DecisionTreeClassifier(min_samples_split = 6,\n",
    "                             min_samples_leaf = 10,\n",
    "                             max_features = 'sqrt')\n",
    "\n",
    "#Learning\n",
    "dt1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "dt1_pred = dt1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,dt1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'n_estimators': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "rf1 = RandomForestClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[10,15,20,25,30],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model16 = RandomizedSearchCV(rf1, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model16.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model16.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834564958283671\n"
     ]
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(criterion='entropy',\n",
    "                             n_estimators = 25,\n",
    "                             min_samples_split = 5,\n",
    "                             min_samples_leaf = 3)\n",
    "\n",
    "#Learning\n",
    "rf1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "rf1_pred = rf1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,rf1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'subsample': 0.8, 'objective': 'binary:logistic', 'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "xgb1 = XGBClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'objective' : ['binary:logistic'],\n",
    "          'learning_rate' : [0.05, 0.1, 0.5, 1],\n",
    "          'max_depth' : [3,5,7],\n",
    "          'n_estimators':[100,400,700,1000],\n",
    "          'subsample' : [0.7, 0.8, 0.9],\n",
    "          'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model17 = RandomizedSearchCV(xgb1, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model17.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model17.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8626936829558999\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate =0.1,\n",
    "                     n_estimators=700,\n",
    "                     max_depth=7,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.9,\n",
    "                     objective= 'binary:logistic')\n",
    "\n",
    "#Learning\n",
    "xgb1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "xgb1_pred = xgb1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,xgb1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'subsample': 0.7, 'n_estimators': 500, 'min_samples_split': 30, 'min_samples_leaf': 20, 'max_features': 'auto', 'max_depth': 7, 'loss': 'deviance', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "gb1 = GradientBoostingClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'loss': ['deviance'],\n",
    "          'learning_rate' : [0.05, 0.1, 0.25, 0.5, 0.75, 1],\n",
    "          'n_estimators':[100, 300, 500, 1000],\n",
    "          'subsample' : [0.7, 0.8, 0.9],\n",
    "          'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'max_depth' : [3,5,7],\n",
    "          'min_samples_leaf':[5,10,20],\n",
    "          'min_samples_split':[20,30]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model18 = RandomizedSearchCV(gb1, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model18.fit(X_tr1,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model18.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8541120381406436\n"
     ]
    }
   ],
   "source": [
    "gb1 = GradientBoostingClassifier(loss='deviance',\n",
    "                                 learning_rate = 0.1,\n",
    "                                 n_estimators = 500,\n",
    "                                 subsample = 0.7 ,\n",
    "                                 max_features = 'auto',\n",
    "                                 max_depth = 7,\n",
    "                                 min_samples_split = 30,\n",
    "                                 min_samples_leaf = 20)\n",
    "\n",
    "#Learning\n",
    "gb1.fit(X_tr1, Y_train)\n",
    "\n",
    "#Prediction\n",
    "gb1_pred = gb1.predict(X_te1)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,gb1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET 2 - Encoding text features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(16777, 2000) (16777,)\n",
      "(4195, 2000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer(min_df=10, max_features=2000)\n",
    "tfv.fit(X_train['title'].values)\n",
    "\n",
    "# we use the fitted TfidfVectorizer to convert the text to vector\n",
    "X_train_title_tfv = tfv.transform(X_train['title'].values)\n",
    "X_test_title_tfv = tfv.transform(X_test['title'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_title_tfv.shape, Y_train.shape)\n",
    "print(X_test_title_tfv.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(16777, 7000) (16777,)\n",
      "(4195, 7000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer(min_df=10, max_features=7000,stop_words = 'english')\n",
    "tfv.fit(X_train['abstract'].values)\n",
    "\n",
    "# we use the fitted TfidfVectorizer to convert the text to vector\n",
    "X_train_abstract_tfv = tfv.transform(X_train['abstract'].values)\n",
    "X_test_abstract_tfv = tfv.transform(X_test['abstract'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_abstract_tfv.shape, Y_train.shape)\n",
    "print(X_test_abstract_tfv.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix 1\n",
      "(16777, 9000) (16777,)\n",
      "(4195, 9000) (4195,)\n"
     ]
    }
   ],
   "source": [
    "# --- Concatinating all the features\n",
    "\n",
    "X_tr2 = hstack((X_train_title_tfv, X_train_abstract_tfv)).tocsr()\n",
    "X_te2 = hstack((X_test_title_tfv, X_test_abstract_tfv)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix 1\")\n",
    "print(X_tr2.shape, Y_train.shape)\n",
    "print(X_te2.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling on TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'weights': 'distance', 'n_neighbors': 9, 'leaf_size': 1, 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[5,9,15,21],\n",
    "          'leaf_size':[1,2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute']}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model21 = RandomizedSearchCV(knn1, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "model21.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model21.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8085816448152563\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(weights = 'distance', n_neighbors =9, n_jobs= -1, leaf_size= 1, algorithm= 'auto')\n",
    "\n",
    "knn2.fit(X_tr2,Y_train)\n",
    "\n",
    "knn2_pred = knn2.predict(X_te2)\n",
    "\n",
    "print(accuracy_score(Y_test,knn2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'penalty': 'l2', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "          'C': [0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model22 = RandomizedSearchCV(lr2, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "model22.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model22.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412395709177592\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(penalty = 'l2', C = 0.1)\n",
    "\n",
    "lr2.fit(X_tr2, Y_train)\n",
    "\n",
    "lr2_pred = lr2.predict(X_te2)\n",
    "\n",
    "print(accuracy_score(Y_test,lr2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'kernel': 'rbf', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "svc2 = SVC()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'kernel': ['rbf'],\n",
    "         'C' : [1,3]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model23 = RandomizedSearchCV(svc2, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "#Learning\n",
    "model23.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model23.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8619785458879619\n"
     ]
    }
   ],
   "source": [
    "svc2 = SVC(kernel='rbf', C = 1)\n",
    "\n",
    "#Learning\n",
    "svc2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "svc2_pred = svc2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,svc2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "bnb2 = BernoulliNB()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'alpha': [0.1, 0.25, 0.5,0.75, 1.0, 1.25 ]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model24 = RandomizedSearchCV(bnb2, param_distributions = params,cv = 3, random_state = 34, n_jobs=1)\n",
    "\n",
    "#Learning\n",
    "model24.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model24.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8448152562574494\n"
     ]
    }
   ],
   "source": [
    "bnb2 = BernoulliNB(alpha = 0.1)\n",
    "\n",
    "#Learning\n",
    "bnb2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "bnb2_pred = bnb2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,bnb2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "dt2 = DecisionTreeClassifier()\n",
    "\n",
    "#Hyper Parameters Set\n",
    "params = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model25 = RandomizedSearchCV(dt2, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model25.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model25.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504171632896305\n"
     ]
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(min_samples_split = 6, \n",
    "                             min_samples_leaf = 7, \n",
    "                             max_features = 'auto')\n",
    "\n",
    "#Learning\n",
    "dt2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "dt2_pred = dt2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,dt2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'n_estimators': 30, 'min_samples_split': 7, 'min_samples_leaf': 3, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "rf2 = RandomForestClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[10,15,20,25,30],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model26 = RandomizedSearchCV(rf2, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model26.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model26.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395709177592372\n"
     ]
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators = 30, \n",
    "                             min_samples_split = 7, \n",
    "                             min_samples_leaf = 3, \n",
    "                             criterion = 'entropy')\n",
    "\n",
    "#Learning\n",
    "rf2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "rf2_pred = rf2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,rf2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'subsample': 0.8, 'objective': 'binary:logistic', 'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "xgb2 = XGBClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'objective' : ['binary:logistic'],\n",
    "          'learning_rate' : [0.05, 0.1, 0.5, 1],\n",
    "          'max_depth' : [3,5,7],\n",
    "          'n_estimators':[100,400,700,1000],\n",
    "          'subsample' : [0.7, 0.8, 0.9],\n",
    "          'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model27 = RandomizedSearchCV(xgb2, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model27.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model27.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8529201430274136\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(learning_rate =0.1,\n",
    "                     n_estimators=700,\n",
    "                     max_depth=7,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.9,\n",
    "                     objective= 'binary:logistic')\n",
    "\n",
    "#Learning\n",
    "xgb2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "xgb2_pred = xgb2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,xgb2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'subsample': 0.7, 'n_estimators': 1000, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 5, 'loss': 'deviance', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#making the instance\n",
    "gb2 = GradientBoostingClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'loss': ['deviance'],\n",
    "          'learning_rate' : [0.05, 0.1, 0.5, 1],\n",
    "          'n_estimators':[100, 300, 500, 1000],\n",
    "          'subsample' : [0.7, 0.8, 0.9],\n",
    "          'max_features': ['auto', 'sqrt'],\n",
    "          'max_depth' : [3,5,7],\n",
    "          'min_samples_leaf':[5,10],\n",
    "          'min_samples_split':[20,30]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model28 = RandomizedSearchCV(gb2, param_distributions = params, cv = 3, random_state = 34, n_jobs=-1)\n",
    "\n",
    "#Learning\n",
    "model28.fit(X_tr2,Y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model28.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8529201430274136\n"
     ]
    }
   ],
   "source": [
    "gb2 = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                 learning_rate = 0.1,\n",
    "                                 n_estimators = 1000,\n",
    "                                 subsample = 0.7 ,\n",
    "                                 max_features = 'sqrt',\n",
    "                                 max_depth = 5,\n",
    "                                 min_samples_split = 30,\n",
    "                                 min_samples_leaf = 5)\n",
    "\n",
    "#Learning\n",
    "gb2.fit(X_tr2, Y_train)\n",
    "\n",
    "#Prediction\n",
    "gb2_pred = gb2.predict(X_te2)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(accuracy_score(Y_test,gb2_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
