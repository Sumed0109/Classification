{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
    "\n",
    "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
    "\n",
    "Data - Overall collection of tweets was split in the ratio of 65:35 into training and testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Files\n",
    "\n",
    "train.csv - For training the models, we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.\n",
    "\n",
    "test_tweets.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric:\n",
    "\n",
    "The metric used for evaluating the performance of classification model would be F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9db4adb75ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m from .compat import (\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInt64Index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mPANDAS_INSTALLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCUDF_INSTALLED\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_client\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAggregation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     from .io import (\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mfrom_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mfrom_pandas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mto_records\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_fwf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_hdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_hdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_sql_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_sql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\csv.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclear_known_categories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplementations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Load the libraries which will be used.\n",
    "\n",
    "import re    \n",
    "import nltk  \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  \n",
    "from wordcloud import WordCloud \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "from prettytable import PrettyTable\n",
    "import gensim\n",
    "from nltk.stem.porter import * \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s read train and test datasets.\n",
    "\n",
    "train  = pd.read_csv('train_E6oV3lV.csv') \n",
    "test = pd.read_csv('test_tweets_anuFYb8.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s check out a few non racist/sexist tweets.\n",
    "\n",
    "train[train['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check out a few racist/sexist tweets.\n",
    "\n",
    "train[train['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s check dimensions of the train and test dataset.\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s have a glimpse at label-distribution in the train dataset.\n",
    "\n",
    "train[\"label\"].value_counts()\n",
    "\n",
    "#In the train dataset, we have 2,242 (~7%) tweets labeled as racist or sexist, and 29,720 (~93%) tweets \n",
    "# labeled as non racist/sexist. So, it is an imbalanced classification challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will check the distribution of length of the tweets, in terms of words, in both train and test data.\n",
    "\n",
    "length_train = train['tweet'].str.len() \n",
    "length_test = test['tweet'].str.len() \n",
    "plt.hist(length_train, bins=20, label=\"train_tweets\") \n",
    "plt.hist(length_test, bins=20, label=\"test_tweets\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we begin cleaning, let’s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data.\n",
    "\n",
    "combi = train.append(test, ignore_index=True) \n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given below is a user-defined function to remove unwanted text patterns from the tweets.\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be following the steps below to clean the raw tweets in out data.\n",
    "\n",
    "We will remove the twitter handles as they are already masked as @user due to privacy concerns. These twitter handles hardly give any information about the nature of the tweet.\n",
    "\n",
    "We will also get rid of the punctuations, numbers and even special characters since they wouldn’t help in differentiating different types of tweets.\n",
    "\n",
    "Most of the smaller words do not add much value. For example, ‘pdx’, ‘his’, ‘all’. So, we will try to remove them as well from our data.\n",
    "\n",
    "Lastly, we will normalize the text data. For example, reducing terms like loves, loving, and lovable to their base word, i.e., ‘love’.are often used in the same context. If we can reduce them to their root word, which is ‘love’. It will help in reducing the total number of unique words in our data without losing a significant amount of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Removing Twitter Handles (@user)\n",
    "\n",
    "#Let’s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed “@[]*” as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ‘@’.\n",
    "\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Removing Punctuations, Numbers, and Special Characters\n",
    "\n",
    "# Here we will replace everything except characters and hashtags with spaces. \n",
    "# The regular expression “[^a-zA-Z#]” means anything except alphabets and ‘#’.\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") \n",
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Removing Short Words\n",
    "\n",
    "#We have to be a little careful here in selecting the length of the words which we want to remove. \n",
    "#So, I have decided to remove all the words having length 3 or less. \n",
    "#For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#Let’s take another look at the first few rows of the combined dataframe.\n",
    "\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Text Normalization\n",
    "\n",
    "# Here we will use nltk’s PorterStemmer() function to normalize the tweets. \n",
    "# But before that we will have to tokenize the tweets. \n",
    "# Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "tokenized_tweet.head()\n",
    "                                                           \n",
    "# Now we can normalize the tokenized tweets.\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "# Now let’s stitch these tokens back together. It can easily be done using nltk’s MosesDetokenizer function.\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \n",
    "combi['tidy_tweet'] = tokenized_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the cleaned tweets. \n",
    "\n",
    "# A) Understanding the common words used in the tweets: WordCloud\n",
    "\n",
    "# Now I want to see how well the given sentiments are distributed across the train dataset. \n",
    "# One way to accomplish this task is by understanding the common words by plotting wordclouds.\n",
    "\n",
    "# A wordcloud is a visualization wherein the most frequent words appear in large size and \n",
    "# the less frequent words appear in smaller sizes.\n",
    "\n",
    "# Let’s visualize all the words our data using the wordcloud plot.\n",
    "\n",
    "all_words = ' '.join([text for text in combi['tidy_tweet']]) \n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n",
    "\n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "#We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Words in non racist/sexist tweets\n",
    "\n",
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "# Most of the frequent words are compatible with the sentiment, i.e, non-racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Racist/Sexist Tweets\n",
    "\n",
    "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]]) \n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words) \n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "# As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D) Understanding the impact of Hashtags on tweets sentiment\n",
    "\n",
    "# Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments.\n",
    "\n",
    "# function to collect hashtags \n",
    "\n",
    "def hashtag_extract(x):    \n",
    "    hashtags = []    \n",
    "    # Loop over the words in the tweet    \n",
    "    for i in x:        \n",
    "        ht = re.findall(r\"#(\\w+)\", i)        \n",
    "        hashtags.append(ht)     \n",
    "    return hashtags\n",
    "\n",
    "# extracting hashtags from non racist/sexist tweets \n",
    "\n",
    "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0]) \n",
    "\n",
    "# extracting hashtags from racist/sexist tweets \n",
    "\n",
    "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1]) \n",
    "\n",
    "# unnesting list \n",
    "HT_regular = sum(HT_regular,[]) \n",
    "HT_negative = sum(HT_negative,[])\n",
    "\n",
    "# Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top ‘n’ hashtags. So, first let’s check the hashtags in the non-racist/sexist tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Racist/Sexist Tweets\n",
    "\n",
    "a = nltk.FreqDist(HT_regular) \n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())}) \n",
    "\n",
    "# selecting top 20 most frequent hashtags     \n",
    "\n",
    "d = d.nlargest(columns=\"Count\", n = 20) \n",
    "plt.figure(figsize=(16,5)) \n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\") \n",
    "ax.set(ylabel = 'Count') \n",
    "plt.show()\n",
    "\n",
    "#All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let’s check the most frequent hashtags appearing in the racist/sexist tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racist/Sexist Tweets\n",
    "\n",
    "b = nltk.FreqDist(HT_negative) \n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()),'Count': list(b.values())}) \n",
    "\n",
    "# selecting top 20 most frequent hashtags \n",
    "\n",
    "e = e.nlargest(columns=\"Count\", n = 20)   \n",
    "plt.figure(figsize=(16,5)) \n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "\n",
    "# As expected, most of the terms are negative with a few neutral terms as well. So, it’s not a bad idea to keep these hashtags in our data as they contain useful information. Next, we will try to extract features from the tokenized tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to feature conversion \n",
    "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Bag of Words, TF-IDF and Word2Vec Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words Features.\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet']) \n",
    "bow.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet']) \n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Embeddings\n",
    "\n",
    "# Let’s train a Word2Vec model on our corpus.\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)\n",
    "\n",
    "# Preparing Vectors for Tweets\n",
    "\n",
    "# Since our data contains tweets and not just words, we’ll have to figure out a way to use the word vectors \n",
    "# from word2vec model to create vector representation for an entire tweet. \n",
    "# There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. \n",
    "# The length of the resultant vector will be the same, i.e. 200. \n",
    "# We will repeat the same process for all the tweets in our data and obtain their vectors. \n",
    "# Now we have 200 word2vec features for our data.\n",
    "\n",
    "# We will use the below function to create a vector for each tweet \n",
    "# by taking the average of the vectors of the words present in the tweet.\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary                                     \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Preparing word2vec feature set…\n",
    "# Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features.\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "\n",
    "wordvec_df.shape \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first try to fit the logistic regression model on the Bag-of_Words (BoW) features.\n",
    "\n",
    "# Extracting train and test BoW features \n",
    "\n",
    "train_bow = bow[:31962,:] \n",
    "test_bow = bow[31962:,:] \n",
    "\n",
    "# splitting data into training and validation set \n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],random_state=42,test_size=0.3)\n",
    "\n",
    "lreg = LogisticRegression() \n",
    "\n",
    "# training the model \n",
    "lreg.fit(xtrain_bow, ytrain) \n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "\n",
    "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let’s make predictions for the test dataset and create a submission file.\n",
    "\n",
    "test_pred = lreg.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll follow the same steps as above, but now for the TF-IDF feature set.\n",
    "\n",
    "train_tfidf = tfidf[:31962,:] \n",
    "test_tfidf = tfidf[31962:,:] \n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index] \n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "# training the model \n",
    "lreg.fit(xtrain_tfidf, ytrain) \n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "train_w2v = wordvec_df.iloc[:31962,:] \n",
    "test_w2v = wordvec_df.iloc[31962:,:] \n",
    "\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:] \n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_w2v, ytrain) \n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_bow) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again let’s make predictions for the test dataset and create another submission file.\n",
    "\n",
    "test_pred = svc.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_svm_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', \n",
    "C=1, probability=True).fit(xtrain_tfidf, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag-of-Words Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n",
    "\n",
    "prediction = rf.predict(xvalid_bow) \n",
    "\n",
    "# validation score \n",
    "f1_score(yvalid, prediction)\n",
    "\n",
    "# Let’s make predictions for the test dataset and create another submission file.\n",
    "\n",
    "test_pred = rf.predict(test_bow) \n",
    "test['label'] = test_pred \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_rf_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \n",
    "\n",
    "prediction = rf.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n",
    "\n",
    "prediction = rf.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words Features\n",
    "\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain) \n",
    "\n",
    "prediction = xgb_model.predict(xvalid_bow) \n",
    "f1_score(yvalid, prediction)\n",
    "\n",
    "test_pred = xgb_model.predict(test_bow) \n",
    "test['label'] = test_pred \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_xgb_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) \n",
    "\n",
    "prediction = xgb.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \n",
    "\n",
    "prediction = xgb.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = PrettyTable()\n",
    "tab.field_names = [\"MODEL\", \"BOW (F1 Score)\", \"TFIDF (F1 Score)\", \"Word2Vec (F1 Score)\"]\n",
    "\n",
    "tab.add_row([\"Logistic Regression\", 0.53, 0.54, 0.62])\n",
    "tab.add_row([\"Linear SVM\", 0.50 , 0.51, 0.62])\n",
    "tab.add_row([\"Random Forest\",0.55, 0.56, 0.51])\n",
    "tab.add_row([\"XGBoost\", 0.51, 0.54, 0.64])\n",
    "\n",
    "print(tab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
